{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Huggingface dataset and tokenizer imports\n",
    "from datasets import Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# ### xVal imports\n",
    "from src.utils import make_tokenizer, preprocess, analyze\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    processors,\n",
    "    Tokenizer,\n",
    "    pre_tokenizers,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1043272\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './dataset'\n",
    "\n",
    "ds = Dataset.from_text(data_dir+'/multi_train')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3 times 7 is 21'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry = 0 # index for each row in the dataset\n",
    "ds['text'][entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens=[\"[END]\", \"[MASK]\", \"[PAD]\", \"[NUM]\"]\n",
    "full_vocab = {}\n",
    "vocab_words = ['times', 'is']\n",
    "tokenizer = Tokenizer(models.BPE(vocab=full_vocab, merges=[]))\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "tokenizer.add_tokens(vocab_words)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.save('./multi_tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"./multi_tokenizer.json\",\n",
    "    bos_token=\"[END]\", # beginning of sentence\n",
    "    eos_token=\"[END]\", # end of sentence\n",
    "    mask_token=\"[MASK]\", # mask token\n",
    "    pad_token=\"[PAD]\", # pad token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 times 7 is 21\n"
     ]
    }
   ],
   "source": [
    "print(ds['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+3.00e+0times+7.00e+0is+2.10e+1\n"
     ]
    }
   ],
   "source": [
    "x = preprocess.convert_num_string(ds['text'][0], sigfigs=3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [3, 4, 3, 5, 3]\n",
      "numbers: [ 3  1  7  1 21]\n"
     ]
    }
   ],
   "source": [
    "tokenized_x = preprocess.tokenize_fnc(ds['text'][0], tokenizer)\n",
    "print('input_ids:', tokenized_x['input_ids'])\n",
    "print('numbers:', tokenized_x['numbers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=30): 100%|██████████| 1043272/1043272 [01:09<00:00, 15101.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting tokenization...\")\n",
    "tokenize_lambda = lambda x: preprocess.tokenize_fnc(x, tokenizer)\n",
    "tokenized_ds = ds.map(\n",
    "    tokenize_lambda,\n",
    "    batched=False,\n",
    "    num_proc=30,\n",
    "    remove_columns=[\"text\"],\n",
    "    load_from_cache_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NUM] --> token #3\n"
     ]
    }
   ],
   "source": [
    "num_token = tokenizer.encode(\"[NUM]\")[0]\n",
    "print(f\"[NUM] --> token #{num_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'numbers', 'len'],\n",
       "    num_rows: 1043272\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 3, 5, 3]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_token_ids = tokenized_ds[entry]['input_ids']\n",
    "x_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 7, 1, 21]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_num = tokenized_ds[entry]['numbers']\n",
    "x_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[NUM]', 'times', '[NUM]', 'is', '[NUM]']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(x) for x in x_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1043272/1043272 [00:00<00:00, 1827863.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds.save_to_disk(data_dir+'/multi_train_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ds(ds, tokenizer):\n",
    "    tokenize_lambda = lambda x: preprocess.tokenize_fnc(x, tokenizer)\n",
    "    tokenized_ds = ds.map(\n",
    "        tokenize_lambda,\n",
    "        batched=False,\n",
    "        num_proc=30,\n",
    "        remove_columns=[\"text\"],\n",
    "        load_from_cache_file=False,\n",
    "    )\n",
    "    return tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 8 examples [00:00, 1524.79 examples/s]\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Map (num_proc=8): 100%|██████████| 8/8 [00:00<00:00, 58.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8/8 [00:00<00:00, 1417.17 examples/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "Generating train split: 81 examples [00:00, 26455.27 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 81/81 [00:00<00:00, 270.24 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 81/81 [00:00<00:00, 12978.52 examples/s]\n",
      "Generating train split: 810 examples [00:00, 310206.92 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 810/810 [00:00<00:00, 2300.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 810/810 [00:00<00:00, 61019.56 examples/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.18it/s]\n",
      "Generating train split: 810 examples [00:00, 236669.19 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 810/810 [00:00<00:00, 2300.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 810/810 [00:00<00:00, 118814.65 examples/s]\n",
      "Generating train split: 8100 examples [00:00, 1428673.78 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 8100/8100 [00:00<00:00, 10412.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8100/8100 [00:00<00:00, 637683.47 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1584669.79 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 12130.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 493405.74 examples/s]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.28s/it]\n",
      "Generating train split: 8100 examples [00:00, 1406610.46 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 8100/8100 [00:00<00:00, 10292.34 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8100/8100 [00:00<00:00, 380144.15 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1576948.30 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 12069.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 476729.47 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1262857.69 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 11975.32 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 335490.64 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1023540.21 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 12093.35 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 709388.83 examples/s]\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.52s/it]\n",
      "Generating train split: 12500 examples [00:00, 1426672.83 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 12078.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 456589.48 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1488946.95 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 12067.59 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 466730.76 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1475828.29 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 12034.24 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 483718.53 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1317472.04 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 12043.92 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 471392.99 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1283446.76 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 11880.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 635901.42 examples/s]\n",
      "100%|██████████| 5/5 [00:07<00:00,  1.60s/it]\n",
      "Generating train split: 9 examples [00:00, 4335.94 examples/s]\n",
      "num_proc must be <= 9. Reducing num_proc to 9 for dataset of size 9.\n",
      "Map (num_proc=9): 100%|██████████| 9/9 [00:00<00:00, 60.36 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 9/9 [00:00<00:00, 2239.22 examples/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "Generating train split: 81 examples [00:00, 37162.40 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 81/81 [00:00<00:00, 264.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 81/81 [00:00<00:00, 439.90 examples/s]\n",
      "Generating train split: 810 examples [00:00, 318674.26 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 810/810 [00:00<00:00, 2265.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 810/810 [00:00<00:00, 116464.51 examples/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\n",
      "Generating train split: 810 examples [00:00, 248256.21 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 810/810 [00:00<00:00, 2250.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 810/810 [00:00<00:00, 123178.50 examples/s]\n",
      "Generating train split: 8100 examples [00:00, 1433133.49 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 8100/8100 [00:00<00:00, 10385.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8100/8100 [00:00<00:00, 583462.64 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1537636.74 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 12034.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 459942.10 examples/s]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.25s/it]\n",
      "Generating train split: 8100 examples [00:00, 1458982.32 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 8100/8100 [00:00<00:00, 10305.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8100/8100 [00:00<00:00, 426721.54 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1278875.99 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 12032.55 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 467142.46 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1561031.38 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 11830.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 479616.52 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1468511.57 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 11955.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 470770.78 examples/s]\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.54s/it]\n",
      "Generating train split: 12500 examples [00:00, 1075131.75 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 11954.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 350148.60 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1571748.06 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 11969.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 454511.41 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1504758.62 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 11822.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 348646.74 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 983839.37 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 11903.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 793918.65 examples/s]\n",
      "Generating train split: 12500 examples [00:00, 1400042.73 examples/s]\n",
      "Map (num_proc=30): 100%|██████████| 12500/12500 [00:01<00:00, 11989.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12500/12500 [00:00<00:00, 438386.22 examples/s]\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "num_digit = 5\n",
    "digits = list(range(1, num_digit + 1))\n",
    "\n",
    "for split in ['val', 'test']:\n",
    "    for a_num_digit in digits:\n",
    "        for b_num_digit in tqdm(digits[:a_num_digit]):\n",
    "            name = f'multi_{split}_{a_num_digit}_by_{b_num_digit}'\n",
    "            ds = Dataset.from_text(f'{data_dir}/{name}')\n",
    "            tokenized_ds = tokenize_ds(ds, tokenizer)\n",
    "            tokenized_ds.save_to_disk(f'{data_dir}/{name}_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'numbers', 'len'],\n",
       "    num_rows: 1043272\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = data_dir+'/multi_train_tokenized'\n",
    "train_tokenized_ds = Dataset.load_from_disk(train_data_path)\n",
    "train_tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbo-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
